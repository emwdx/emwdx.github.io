<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	
	xmlns:georss="http://www.georss.org/georss"
	xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#"
	
	>
<channel>
	<title>
	Comments on: (Students) thinking like computer scientists	</title>
	<atom:link href="/2013/04/19/students-thinking-like-computer-scientists/feed/" rel="self" type="application/rss+xml" />
	<link>/2013/04/19/students-thinking-like-computer-scientists/</link>
	<description>iteration, making, building, and coding in education</description>
	<lastBuildDate>Tue, 23 Jun 2015 22:38:01 +0000</lastBuildDate>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.7.6</generator>
	<item>
		<title>
		By: Do You Know Blue? &#124; Mathy McMatherson		</title>
		<link>/2013/04/19/students-thinking-like-computer-scientists/#comment-197</link>

		<dc:creator><![CDATA[Do You Know Blue? &#124; Mathy McMatherson]]></dc:creator>
		<pubDate>Tue, 23 Jun 2015 22:38:01 +0000</pubDate>
		<guid isPermaLink="false">/?p=1378#comment-197</guid>

					<description><![CDATA[[&#8230;] /2013/04/19/students-thinking-like-computer-scientists/ [&#8230;]]]></description>
			<content:encoded><![CDATA[<p>[&#8230;] <a href="/2013/04/19/students-thinking-like-computer-scientists/" rel="nofollow ugc">/2013/04/19/students-thinking-like-computer-scientists/</a> [&#8230;]</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Bob		</title>
		<link>/2013/04/19/students-thinking-like-computer-scientists/#comment-196</link>

		<dc:creator><![CDATA[Bob]]></dc:creator>
		<pubDate>Fri, 21 Jun 2013 22:05:39 +0000</pubDate>
		<guid isPermaLink="false">/?p=1378#comment-196</guid>

					<description><![CDATA[http://blog.xkcd.com/2010/05/03/color-survey-results/
Something vaguely related.]]></description>
			<content:encoded><![CDATA[<p><a href="http://blog.xkcd.com/2010/05/03/color-survey-results/" rel="nofollow ugc">http://blog.xkcd.com/2010/05/03/color-survey-results/</a><br />
Something vaguely related.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Thanassis		</title>
		<link>/2013/04/19/students-thinking-like-computer-scientists/#comment-195</link>

		<dc:creator><![CDATA[Thanassis]]></dc:creator>
		<pubDate>Mon, 10 Jun 2013 01:48:50 +0000</pubDate>
		<guid isPermaLink="false">/?p=1378#comment-195</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;/2013/04/19/students-thinking-like-computer-scientists/#comment-194&quot;&gt;Evan Weinberg&lt;/a&gt;.

Yes, it&#039;s python :) But I am using a few libraries, like numpy and libsvm.
I would be glad to share my code. I am not sure how readable it is though :) 
&quot;Once a color has been voted on 10 times, its blueness percentage shouldnâ€™t change. The changes you see in the percentage are from new colors entering the database after receiving 10 votes.&quot;
Yes, this is/was my understanding too, but it does not seem to add up. The standings page give you the total number of colours in the database. I am claiming that the accuracy changed *without* the total number of colours changing.
Also there seem to be inaccuracies in the way the rules are evaluated, I left a comment at Dan&#039;s blog (the last comment I believe).]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="/2013/04/19/students-thinking-like-computer-scientists/#comment-194">Evan Weinberg</a>.</p>
<p>Yes, it&#8217;s python ðŸ™‚ But I am using a few libraries, like numpy and libsvm.<br />
I would be glad to share my code. I am not sure how readable it is though ðŸ™‚<br />
&#8220;Once a color has been voted on 10 times, its blueness percentage shouldnâ€™t change. The changes you see in the percentage are from new colors entering the database after receiving 10 votes.&#8221;<br />
Yes, this is/was my understanding too, but it does not seem to add up. The standings page give you the total number of colours in the database. I am claiming that the accuracy changed *without* the total number of colours changing.<br />
Also there seem to be inaccuracies in the way the rules are evaluated, I left a comment at Dan&#8217;s blog (the last comment I believe).</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Evan Weinberg		</title>
		<link>/2013/04/19/students-thinking-like-computer-scientists/#comment-194</link>

		<dc:creator><![CDATA[Evan Weinberg]]></dc:creator>
		<pubDate>Mon, 10 Jun 2013 00:28:39 +0000</pubDate>
		<guid isPermaLink="false">/?p=1378#comment-194</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;/2013/04/19/students-thinking-like-computer-scientists/#comment-193&quot;&gt;Thanassis&lt;/a&gt;.

This is very cool - thanks for your comments, and I&#039;m sorry about the delay in responding. We&#039;re in the final stages of our school year here.&lt;p&gt;

Your steps in actually using the machine learning algorithms from the CS class are interesting to me here - I&#039;d like to know the details. What programming language did you use? (PLEASE let it be python!) I find it fascinating that these algorithms can figure out patterns so well when there are multiple variables as in this case.

If I understand Dave&#039;s programming structure correctly, the colors are all voted on 10 times before being entered into the database. When your rule is tested, it is tested against the 10 rules you voted on, and ten other colors from the database. Once a color has been voted on 10 times, its blueness percentage shouldn&#039;t change. The changes you see in the percentage are from new colors entering the database after receiving 10 votes.

I&#039;ll need some more time to sift through your process, but your analysis is exactly the sort of thing I want to learn more about. Binary classification is really cool, and I like the idea of being able to train a learning algorithm from saying yes/no based on a set of examples. Thanks for participating!]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="/2013/04/19/students-thinking-like-computer-scientists/#comment-193">Thanassis</a>.</p>
<p>This is very cool &#8211; thanks for your comments, and I&#8217;m sorry about the delay in responding. We&#8217;re in the final stages of our school year here.</p>
<p>Your steps in actually using the machine learning algorithms from the CS class are interesting to me here &#8211; I&#8217;d like to know the details. What programming language did you use? (PLEASE let it be python!) I find it fascinating that these algorithms can figure out patterns so well when there are multiple variables as in this case.</p>
<p>If I understand Dave&#8217;s programming structure correctly, the colors are all voted on 10 times before being entered into the database. When your rule is tested, it is tested against the 10 rules you voted on, and ten other colors from the database. Once a color has been voted on 10 times, its blueness percentage shouldn&#8217;t change. The changes you see in the percentage are from new colors entering the database after receiving 10 votes.</p>
<p>I&#8217;ll need some more time to sift through your process, but your analysis is exactly the sort of thing I want to learn more about. Binary classification is really cool, and I like the idea of being able to train a learning algorithm from saying yes/no based on a set of examples. Thanks for participating!</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Thanassis		</title>
		<link>/2013/04/19/students-thinking-like-computer-scientists/#comment-193</link>

		<dc:creator><![CDATA[Thanassis]]></dc:creator>
		<pubDate>Fri, 31 May 2013 05:28:33 +0000</pubDate>
		<guid isPermaLink="false">/?p=1378#comment-193</guid>

					<description><![CDATA[I found a mistake in my code, that explains the poor performance of the 4-degree plane. After correction, I should have seen 93.75%. Still the webpage give me 91.36%. My calculations are based on all 4304 colours currently in the database. Maybe there are rounding errors as many terms in my rule are very small (10^-9)]]></description>
			<content:encoded><![CDATA[<p>I found a mistake in my code, that explains the poor performance of the 4-degree plane. After correction, I should have seen 93.75%. Still the webpage give me 91.36%. My calculations are based on all 4304 colours currently in the database. Maybe there are rounding errors as many terms in my rule are very small (10^-9)</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: Thanassis		</title>
		<link>/2013/04/19/students-thinking-like-computer-scientists/#comment-192</link>

		<dc:creator><![CDATA[Thanassis]]></dc:creator>
		<pubDate>Fri, 31 May 2013 04:21:02 +0000</pubDate>
		<guid isPermaLink="false">/?p=1378#comment-192</guid>

					<description><![CDATA[Hi Evan, great idea indeed! I first found out about this from Dan&#039;s website and I participated in the first round of the contest. When I realised that this was a real Machine Learning problem (with a complex/noisy target function) and a large number of &quot;hidden&quot; data points I was really excited! A few months ago I took Caltech&#039;s online course &quot;Learning From Data&quot; which I really enjoyed (and worked hard for). &quot;Do you know blue&quot; was an excellent example to revisit some of the material and test my knowledge in a practical problem.
I run the perceptron algorithm too, regression with non-linear transforms, and even support vectors machines (SVM) with RBF kernel (probably all these sound like gibberish to you). The main problem was the few data points given (30 points/colours to quickly test your rule). With 30 points you cannot hope for much in machine learning. I tried to gather more points by doing the tests again, but after a few days I was getting the same blue points. So at the end I had 140 points/colours of which 44 where blue.
I run different non-linear regressions and even SVM. The best approach was regression with a cubic non-linear transform. It scored 2nd in the overall board and it seemed to come closer to 1st with more points added in the database. The SVM approach did not seem to work, as it was merely fitting the 140 points and would not generalize well to the overall database.
The first contest ended and then the site opened again after a few days. I decided to play a bit more with it. I saw that the original points/colours were kept. So I entered the same cubic rule and it scored slightly better than before. Then I thought I&#039;d try something major. You have a page (/blueis) that you give all the colors in the database in little boxes, separated in two regions blue and not blue. I saved these regions as images and then wrote a program to parse them and get r,g,b, values for all the different colours. So now I have the entire database (4090 points at the time). Now I could run the algorithm on the entire database and see what are the best results I could get. This is not really machine learning, it is more like fitting :)
Curiously enough the cubic regression on the 4090 points works slightly worse than the cubic regression on the 140 points I initially had!
Then I noticed some things that made me scratch my head even more.
I tried a 4th degree plane to separate the blue from non blue, as I was expecting it would do better than cubic with 4090 points. It did. With my calculations it was giving me an accuracy of 93.4%. But when I applied the rule with your website I got 80%. I assumed 80% is what you get if your rule compute always false. So maybe my rule was not parsed/calculated correctly (it is a long rule afterall). I checked this assumption by entering an always-false rule (r=300) and noticed that although it was close, it was not identical.
Moreover I start noticing that accuracy scores were changing in the standings *without* the number of colours changing. The number of responses are changing, but I assume, that the colours in the database are already fixed, i.e, when a color enters the database, it does not appear in the test and it does not change its blueness value.
If you have thoughts on the last points I&#039;d love to hear them.]]></description>
			<content:encoded><![CDATA[<p>Hi Evan, great idea indeed! I first found out about this from Dan&#8217;s website and I participated in the first round of the contest. When I realised that this was a real Machine Learning problem (with a complex/noisy target function) and a large number of &#8220;hidden&#8221; data points I was really excited! A few months ago I took Caltech&#8217;s online course &#8220;Learning From Data&#8221; which I really enjoyed (and worked hard for). &#8220;Do you know blue&#8221; was an excellent example to revisit some of the material and test my knowledge in a practical problem.<br />
I run the perceptron algorithm too, regression with non-linear transforms, and even support vectors machines (SVM) with RBF kernel (probably all these sound like gibberish to you). The main problem was the few data points given (30 points/colours to quickly test your rule). With 30 points you cannot hope for much in machine learning. I tried to gather more points by doing the tests again, but after a few days I was getting the same blue points. So at the end I had 140 points/colours of which 44 where blue.<br />
I run different non-linear regressions and even SVM. The best approach was regression with a cubic non-linear transform. It scored 2nd in the overall board and it seemed to come closer to 1st with more points added in the database. The SVM approach did not seem to work, as it was merely fitting the 140 points and would not generalize well to the overall database.<br />
The first contest ended and then the site opened again after a few days. I decided to play a bit more with it. I saw that the original points/colours were kept. So I entered the same cubic rule and it scored slightly better than before. Then I thought I&#8217;d try something major. You have a page (/blueis) that you give all the colors in the database in little boxes, separated in two regions blue and not blue. I saved these regions as images and then wrote a program to parse them and get r,g,b, values for all the different colours. So now I have the entire database (4090 points at the time). Now I could run the algorithm on the entire database and see what are the best results I could get. This is not really machine learning, it is more like fitting ðŸ™‚<br />
Curiously enough the cubic regression on the 4090 points works slightly worse than the cubic regression on the 140 points I initially had!<br />
Then I noticed some things that made me scratch my head even more.<br />
I tried a 4th degree plane to separate the blue from non blue, as I was expecting it would do better than cubic with 4090 points. It did. With my calculations it was giving me an accuracy of 93.4%. But when I applied the rule with your website I got 80%. I assumed 80% is what you get if your rule compute always false. So maybe my rule was not parsed/calculated correctly (it is a long rule afterall). I checked this assumption by entering an always-false rule (r=300) and noticed that although it was close, it was not identical.<br />
Moreover I start noticing that accuracy scores were changing in the standings *without* the number of colours changing. The number of responses are changing, but I assume, that the colours in the database are already fixed, i.e, when a color enters the database, it does not appear in the test and it does not change its blueness value.<br />
If you have thoughts on the last points I&#8217;d love to hear them.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		By: dy/dan &#187; Blog Archive &#187; Great Lessons: Evan Weinberg&#8217;s &#8220;Do You Know Blue?&#8221;		</title>
		<link>/2013/04/19/students-thinking-like-computer-scientists/#comment-191</link>

		<dc:creator><![CDATA[dy/dan &#187; Blog Archive &#187; Great Lessons: Evan Weinberg&#8217;s &#8220;Do You Know Blue?&#8221;]]></dc:creator>
		<pubDate>Thu, 23 May 2013 22:13:33 +0000</pubDate>
		<guid isPermaLink="false">/?p=1378#comment-191</guid>

					<description><![CDATA[[&#8230;] Weinberg posted &#034;(Students) Thinking Like Computer Scientists&#034; a month ago and the lesson idea haunted me since. It realizes the promise of digital, networked [&#8230;]]]></description>
			<content:encoded><![CDATA[<p>[&#8230;] Weinberg posted &quot;(Students) Thinking Like Computer Scientists&quot; a month ago and the lesson idea haunted me since. It realizes the promise of digital, networked [&#8230;]</p>
]]></content:encoded>
		
			</item>
	</channel>
</rss>
